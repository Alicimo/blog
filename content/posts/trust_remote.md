+++
title = 'Losing Trust Remote'
date = 2024-12-28
draft = false
+++

Currently, like a large portion of data scientists, I am working on a Generative AI (read: NLP) product. This has drawn me into the eco-system of HuggingFace and their excellent python packages. However, regardless of the high-quality documentation, things may still be unclear. This was the case with the `trust_remote_code` flag, which defaults to `False`. The docstring for this flag is

> **trust_remote_code** (_bool__,_ _optional_) â€“ Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to True for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

Given the word "remote" in the flag, I presumed that this flag is only relevant for models downloaded from HF (or your local HF cache), i.e., those loaded via `SentenceTransformer("[MODEL_NAME]", trust_remote_code=True)`. This is further reinforced when by the mention of "repositories" and "code present on the Hub". However, as I'll show below, this is not the case.

In the current product, we would download the model in the first stage and use it to process some data. We would then use the same model again at a later stage within the product. Until now, we used base-models, meaning that we could use the above call to load the model at both points, the cache being used on the second requests. Also, our model choice meant we had to set `trust_remote_code = True`.

In my feature, I was changing this to save the model at the given location locally, which would specified in the second call. This was a preliminary change to allow for fine-tuning the model, and therefore altering it from the version supplied by HF, in the near future. This led to the following code:

```
# STAGE 1
model_1 = SentenceTransformer("[MODEL_NAME]", trust_remote_code=True)
...(data processing)...
model_1.save("local/dir")

# STAGE 2
model_2 = SentenceTransformer("local/dir")
...
```

The first thing to note is that the above pipeline does not produce any warnings and/or errors when loading the model the second time. Further, `model_2` can be used as expected; namely the remaining code in stage two runs to completion. However, I found that the outcome was drastically different. Namely,

```
model_1.embed(["Ah! A cat"]) == model_2.embed(["Ah! A cat"])
# False
```

Embeddings generated by `model_2` are different! This change drastically changed the performance of our product because we had processed the data using `model_1`. Thankfully our pipeline caught this change and nothing made it to production. Adding the removed flag reverts the behaviour to that expected.

```
model_3 = SentenceTransformer("local/dir", trust_remote_code=True)
model_1.embed(["Ah! A cat"]) == model_3.embed(["Ah! A cat"])
# True
```

The underlying reason for this is documented [here](https://huggingface.co/docs/transformers/custom_models#building-custom-models). The `remote_code_true` flag indicates whether a model is fully encapsulated by the Transformers package. Models that rely on code not present in this package, i.e., custom functions or additional packages, require one requires the flag be set.  Given this, I do not understand why `model_2` does not raise errors/warnings when loaded without said flag or when it produces embeddings.  If it does not need the remote code to produce embeddings, why do I need it when loading `model_1`?

While simple on the surface, this bug, wrapped up with other changes on a code base I was unfamiliar with, cost me more time than I would like to admit. If google has served you this blog post in your debugging search, I hope it helps.